{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec55bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose, OneOf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# project-specific imports\n",
    "from archs import gigatime\n",
    "from losses import *\n",
    "from utils import *\n",
    "from prov_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d83d5",
   "metadata": {},
   "source": [
    "Argument Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f929e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true', '1')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--name', default=\"gigatime_training\")  # experiment name\n",
    "    parser.add_argument('--output_dir', default=\"./scratch\")  # directory to save outputs\n",
    "    parser.add_argument('--gpu_ids', nargs='+', type=int)  # GPU IDs to use for training\n",
    "    parser.add_argument('--metadata', default=\"./../data/full_metadata.csv\")  # path to metadata CSV file\n",
    "    parser.add_argument('--tiling_dir', default=\"./../data/gigatime_training_tiles/\")  # directory containing tiled images\n",
    "    parser.add_argument('--epochs', default=1, type=int)  # number of training epochs\n",
    "    parser.add_argument('--batch_size', default=32, type=int)  # batch size for training\n",
    "\n",
    "    # model\n",
    "    parser.add_argument('--arch', default='NestedUNet')  # model architecture\n",
    "    parser.add_argument('--input_channels', default=3, type=int)  # number of input channels\n",
    "    parser.add_argument('--num_classes', default=23, type=int)  # number of output classes\n",
    "    parser.add_argument('--input_w', default=512, type=int)  # input image width to resize to\n",
    "    parser.add_argument('--input_h', default=512, type=int)  # input image height to resize to\n",
    "\n",
    "    # loss\n",
    "    parser.add_argument('--loss', default='BCEDiceLoss')  # loss function to use\n",
    "\n",
    "    # optimizer\n",
    "    parser.add_argument('--optimizer', default='Adam', choices=['Adam', 'SGD'])  # optimizer type\n",
    "    parser.add_argument('--lr', default=1e-3, type=float)  # learning rate\n",
    "    parser.add_argument('--momentum', default=0.9, type=float)  # momentum for SGD optimizer\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)  # weight decay for regularization\n",
    "    parser.add_argument('--nesterov', default=False, type=str2bool)  # enable Nesterov acceleration for SGD\n",
    "\n",
    "    # scheduler\n",
    "    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n",
    "                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])  # learning rate scheduler\n",
    "    parser.add_argument('--min_lr', default=1e-5, type=float)  # minimum learning rate\n",
    "    parser.add_argument('--factor', default=0.1, type=float)  # factor to reduce learning rate\n",
    "    parser.add_argument('--patience', default=2, type=int)  # patience for ReduceLROnPlateau scheduler\n",
    "    parser.add_argument('--milestones', default='1,2', type=str)  # epochs to reduce learning rate for MultiStepLR\n",
    "    parser.add_argument('--gamma', default=2/3, type=float)  # learning rate decay factor\n",
    "    parser.add_argument('--early_stopping', default=-1, type=int)  # early stopping patience (-1 to disable)\n",
    "\n",
    "    parser.add_argument('--num_workers', default=12, type=int)  # number of data loader workers\n",
    "    parser.add_argument('--window_size', type=int, default=256)  # size of cropping window\n",
    "    parser.add_argument('--sampling_prob', type=float, default=0.5)  # sampling probability for training data, this is just to speed up training if we want to debug or train on a less data\n",
    "    parser.add_argument('--val_sampling_prob', type=float, default=0.01)  # sampling probability for validation data, this is just to speed up training if we want to debug or train on a less data\n",
    "    parser.add_argument('--transformer', type=str2bool, default=False)  # enable transformer architecture\n",
    "    parser.add_argument('--sigmoid', type=str2bool, default=True)  # apply sigmoid activation to output\n",
    "    parser.add_argument('--crop', type=str2bool, default=False)  # enable random cropping during training\n",
    "\n",
    "    return edict(vars(parser.parse_args([])))  # use [] so notebook runs\n",
    "\n",
    "config = parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105117bc",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd10e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).cuda()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_correlations(matrix1, matrix2):\n",
    "    \"\"\"\n",
    "    Calculate Pearson and Spearman correlation coefficients between two matrices.\n",
    "\n",
    "    Args:\n",
    "        matrix1 (np.ndarray): The first matrix.\n",
    "        matrix2 (np.ndarray): The second matrix.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing Pearson and Spearman correlation coefficients.\n",
    "    \"\"\"\n",
    "    assert matrix1.shape == matrix2.shape, \"Matrices must have the same shape\"\n",
    "    b, c, h, w = matrix1.shape\n",
    "\n",
    "    pearson_correlations = []\n",
    "    spearman_correlations = []\n",
    "\n",
    "    for channel in range(c):\n",
    "        pearson_corrs = []\n",
    "        spearman_corrs = []\n",
    "\n",
    "        for batch in range(b):\n",
    "            flat_matrix1 = matrix1[batch, channel].flatten()\n",
    "            flat_matrix2 = matrix2[batch, channel].flatten()\n",
    "\n",
    "            # Remove NaN values\n",
    "            valid_indices = ~np.isnan(flat_matrix1.detach().cpu().numpy()) & ~np.isnan(flat_matrix2.detach().cpu().numpy())\n",
    "            flat_matrix1 = flat_matrix1[valid_indices]\n",
    "            flat_matrix2 = flat_matrix2[valid_indices]\n",
    "\n",
    "            if len(flat_matrix1) > 0 and len(flat_matrix2) > 0:\n",
    "                pearson_corr, _ = pearsonr(flat_matrix1.detach().cpu().numpy(), flat_matrix2.detach().cpu().numpy())\n",
    "                spearman_corr, _ = spearmanr(flat_matrix1.detach().cpu().numpy(), flat_matrix2.detach().cpu().numpy())\n",
    "            else:\n",
    "                pearson_corr = np.nan\n",
    "                spearman_corr = np.nan\n",
    "\n",
    "            pearson_corrs.append(pearson_corr)\n",
    "            spearman_corrs.append(spearman_corr)\n",
    "\n",
    "        # Average correlations across the batch dimension\n",
    "        pearson_correlations.append(np.nanmean(pearson_corrs))\n",
    "        spearman_correlations.append(np.nanmean(spearman_corrs))\n",
    "\n",
    "    return pearson_correlations, spearman_correlations\n",
    "    \n",
    "\n",
    "def split_into_boxes(tensor, box_size):\n",
    "    # Get the dimensions of the tensor\n",
    "    batch_size, channels, height, width = tensor.shape\n",
    "    \n",
    "    # Calculate the number of boxes along each dimension\n",
    "    num_boxes_y = height // box_size\n",
    "    num_boxes_x = width // box_size\n",
    "    \n",
    "    # Split the tensor into non-overlapping boxes\n",
    "    boxes = tensor.unfold(2, box_size, box_size).unfold(3, box_size, box_size)\n",
    "    boxes = boxes.contiguous().view(batch_size, channels, num_boxes_y, num_boxes_x, box_size, box_size)\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def count_ones(boxes):\n",
    "    # Count the number of ones in each box\n",
    "    return boxes.sum(dim=(4, 5))\n",
    "\n",
    "\n",
    "\n",
    "def get_box_metrics(pred, mask, box_size):\n",
    "    # Split the images into boxes\n",
    "    pred_boxes = split_into_boxes(pred, box_size)\n",
    "    mask_boxes = split_into_boxes(mask, box_size)\n",
    "    # Count the number of ones in each box\n",
    "    pred_counts = count_ones(pred_boxes)\n",
    "    mask_counts = count_ones(mask_boxes)\n",
    "    \n",
    "    # Calculate precision and MSE for the matrices\n",
    "    mse = ((pred_counts.float() - mask_counts.float()) ** 2).mean(dim=0)    \n",
    "    mean_mse_per_channel = mse.mean(dim=(1,2))\n",
    "\n",
    "    mean_mse = mse.mean().item()\n",
    "\n",
    "    pearson, spearman = calculate_correlations(pred_counts, mask_counts)\n",
    "    \n",
    "    return mean_mse_per_channel, pearson, spearman \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd240b1",
   "metadata": {},
   "source": [
    "Data Loader Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659f94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_loader(data_loader, config, sample_fraction=0.1, deterministic=False, what_split=\"train\"):\n",
    "    # this just samples some fraction of the data in the dataloader so that we can train on a smaller subset for quick testing\n",
    "    dataset = data_loader.dataset\n",
    "    total_size = len(dataset)\n",
    "    sample_size = int(total_size * sample_fraction)\n",
    "\n",
    "    if deterministic:\n",
    "        sample_indices = [i for i in range(sample_size)]\n",
    "    else:\n",
    "        sample_indices = random.sample(range(total_size), sample_size)\n",
    "\n",
    "    subset = Subset(dataset, sample_indices)\n",
    "\n",
    "    if what_split == \"train\":\n",
    "        sample_loader = DataLoader(subset, batch_size=data_loader.batch_size, shuffle=True,\n",
    "            num_workers=config['num_workers'], prefetch_factor=6, drop_last=True)\n",
    "    else:\n",
    "        sample_loader = DataLoader(subset, batch_size=data_loader.batch_size, shuffle=False,\n",
    "            num_workers=config['num_workers'], prefetch_factor=6, drop_last=False)\n",
    "    return sample_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03b4d8",
   "metadata": {},
   "source": [
    "Training & Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ff532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, train_loader, model, criterion, optimizer):\n",
    "    # Initialize average meters to track loss and Pearson correlation metrics\n",
    "    avg_meters = {'loss': AverageMeter(), 'pearson': AverageMeter()}\n",
    "    pearson_per_class_meters = [AverageMeter() for _ in range(config['num_classes'])]\n",
    "    window_size = config['window_size']\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize progress bar for training loop\n",
    "    pbar = tqdm.tqdm(total=len(train_loader))\n",
    "    for input, target, name in train_loader:\n",
    "        # Downsample target by factor of 8, then resize to input dimensions to make the target coarse to discount for any pixel level registration error\n",
    "        downsampled_image = F.interpolate(target, scale_factor=1/8, mode='bilinear', align_corners=False)\n",
    "        target = F.interpolate(downsampled_image, size=(config[\"input_h\"],config[\"input_h\"]), mode='bilinear', align_corners=False)\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # Forward pass through model\n",
    "        output_image = model(input.cuda()).cuda()\n",
    "\n",
    "        # Calculate loss between predicted and target images\n",
    "        loss = criterion(output_image, target)\n",
    "        \n",
    "        # Backpropagation and parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate IoU metrics for overall and per-class evaluation\n",
    "        _, pearson, _ = get_box_metrics(output_image, target, box_size=8)\n",
    "\n",
    "        \n",
    "        # Update per-class Pearson meters\n",
    "        for class_idx, pearson_value in enumerate(pearson):\n",
    "            pearson_per_class_meters[class_idx].update(pearson_value, input.size(0))\n",
    "\n",
    "        # Update average meters with current batch metrics\n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['pearson'].update(np.nanmean(pearson), input.size(0))\n",
    "\n",
    "        # Update progress bar with current metrics\n",
    "        pbar.set_postfix({'loss': avg_meters['loss'].avg, 'pearson': avg_meters['pearson'].avg})\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # Return ordered dictionary with training metrics\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg), ('pearson', avg_meters['pearson'].avg)] +\n",
    "                       [(f'class_{i}', m.avg) for i, m in enumerate(pearson_per_class_meters)])\n",
    "\n",
    "def validate(config, val_loader, model, criterion):\n",
    "    # Initialize average meters to track validation loss and Pearson correlation metrics\n",
    "    avg_meters = {'loss': AverageMeter(), 'pearson': AverageMeter()}\n",
    "    pearson_per_class_meters = [AverageMeter() for _ in range(config['num_classes'])]\n",
    "    window_size = config['window_size']\n",
    "    \n",
    "    # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation for validation (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        # Initialize progress bar for validation loop\n",
    "        pbar = tqdm.tqdm(total=len(val_loader))\n",
    "        for input, target, name in val_loader:\n",
    "            # Downsample target by factor of 8, then resize to input dimensions\n",
    "            downsampled_image = F.interpolate(target, scale_factor=1/8, mode='bilinear', align_corners=False)\n",
    "            target = F.interpolate(downsampled_image, size=(config[\"input_h\"],config[\"input_h\"]), mode='bilinear', align_corners=False)\n",
    "            target = target.cuda()\n",
    "            \n",
    "            # Forward pass through model\n",
    "            output_image = model(input.cuda()).cuda()\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = criterion(output_image, target)\n",
    "            \n",
    "            # Calculate IoU metrics for overall and per-class evaluation\n",
    "            _, pearson, _ = get_box_metrics(output_image, target, box_size=8)\n",
    "            \n",
    "            # Update per-class IoU meters\n",
    "            for class_idx, pearson_value in enumerate(pearson):\n",
    "                pearson_per_class_meters[class_idx].update(pearson_value, input.size(0))\n",
    "\n",
    "            # Update average meters with current batch metrics\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['pearson'].update(np.nanmean(pearson), input.size(0))\n",
    "\n",
    "            # Update progress bar with current metrics\n",
    "            pbar.set_postfix({'loss': avg_meters['loss'].avg, 'pearson': avg_meters['pearson'].avg})\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "    # Return ordered dictionary with validation metrics\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg), ('pearson', avg_meters['pearson'].avg)] +\n",
    "                       [(f'class_{i}', m.avg) for i, m in enumerate(pearson_per_class_meters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649943d0",
   "metadata": {},
   "source": [
    "Model, Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de8a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel names\n",
    "common_channel_list = [\n",
    "    'DAPI','TRITC','Cy5','PD-1_1:200 - Cy5','CD14 - Cy5','CD4 - Cy5','T-bet - Cy5',\n",
    "    'CD34 - Cy5','CD68_1:100 - TRITC','CD16 - Cy5','CD11c - Cy5','CD138 - TRITC',\n",
    "    'CD20 - TRITC','CD3_1:1000 - Cy5','CD8 - TRITC','PD-L1 - Cy5','CK_1:150 - TRITC',\n",
    "    'Ki67_1:150 - TRITC','Tryptase - TRITC','Actin-D - TRITC','Caspase3-D - Cy5',\n",
    "    'PHH3-B - Cy5','Transgelin - TRITC'\n",
    "]\n",
    "\n",
    "# loss\n",
    "if config['loss'] == 'MSELoss':\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "elif config['loss'] == 'BCEWithLogitsLoss':\n",
    "    criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "elif config['loss'] == 'BCEDiceLoss':\n",
    "    criterion = BCEDiceLoss().cuda()\n",
    "else:\n",
    "    criterion = losses.__dict__[config['loss']]().cuda()\n",
    "\n",
    "# model\n",
    "model = gigatime(num_classes=config['num_classes'],\n",
    "                 sigmoid=config[\"sigmoid\"],\n",
    "                 loss_type=config[\"loss\"],\n",
    "                 input_channels=config['input_channels']).cuda()\n",
    "\n",
    "if config[\"gpu_ids\"] and len(config[\"gpu_ids\"]) > 1:\n",
    "    model = nn.DataParallel(model, device_ids=config[\"gpu_ids\"])\n",
    "    print(\"using multiple GPUs\", config[\"gpu_ids\"])\n",
    "\n",
    "# optimizer\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "if config['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n",
    "                          nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n",
    "\n",
    "# scheduler\n",
    "if config['scheduler'] == 'CosineAnnealingLR':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n",
    "elif config['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'],\n",
    "                                                     patience=config['patience'], verbose=1,\n",
    "                                                     min_lr=config['min_lr'])\n",
    "elif config['scheduler'] == 'MultiStepLR':\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                               milestones=[int(e) for e in config['milestones'].split(',')],\n",
    "                                               gamma=config['gamma'])\n",
    "elif config['scheduler'] == 'ConstantLR':\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ba487",
   "metadata": {},
   "source": [
    "Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b1be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n",
      "/tmp/ipykernel_3395014/1532601388.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tile_pair_df_filtered[key] = values\n"
     ]
    }
   ],
   "source": [
    "# Read metadata CSV file containing experiment information\n",
    "metadata = pd.read_csv(config[\"metadata\"])\n",
    "\n",
    "# Convert tiling directory path to Path object for easier file handling\n",
    "tiliting_dir = Path(config[\"tiling_dir\"])\n",
    "\n",
    "# Generate dataframe containing tile pair information from metadata and tiling directory\n",
    "tile_pair_df = generate_tile_pair_df(metadata=metadata, tiling_dir=tiliting_dir)\n",
    "\n",
    "# Filter tile pairs based on image quality metrics, the exact numbers were decided based on empirical analysis as well as suggestions from domain experts:\n",
    "# - Black ratio < 0.3\n",
    "# - Variance > 200\n",
    "# Applied to both COMET and H&E images\n",
    "tile_pair_df_filtered = tile_pair_df[tile_pair_df.apply(\n",
    "    lambda x: (x[\"img_comet_black_ratio\"] < 0.3) &\n",
    "              (x[\"img_comet_variance\"] > 200) &\n",
    "              (x[\"img_he_black_ratio\"] < 0.3) &\n",
    "              (x[\"img_he_variance\"] > 200), axis=1\n",
    ")]\n",
    "\n",
    "# Load segmentation metrics from JSON files for each unique directory\n",
    "dir_names = tile_pair_df_filtered[\"dir_name\"].unique()\n",
    "segment_metric_dict = {}\n",
    "for dir_name in dir_names:\n",
    "    # Read segment metrics JSON file from each directory\n",
    "    with open(os.path.join(dir_name, \"segment_metric.json\"), \"r\") as f:\n",
    "        segment_metric_list = json.load(f)\n",
    "    segment_metric_dict[dir_name] = segment_metric_list\n",
    "\n",
    "# Initialize new columns based on metric keys from first directory's first entry\n",
    "new_columns = {col: [] for col in next(iter(segment_metric_dict[dir_names[0]].values())).keys()}\n",
    "\n",
    "# Populate new columns with metrics for each tile pair\n",
    "for _, row in tile_pair_df_filtered.iterrows():\n",
    "    # Get metrics for current tile pair from corresponding directory\n",
    "    metrics = segment_metric_dict[row[\"dir_name\"]][row[\"pair_name\"]]\n",
    "    # Add each metric value to corresponding column list\n",
    "    for key, value in metrics.items():\n",
    "        new_columns[key].append(value)\n",
    "\n",
    "# Add all metric columns to the filtered dataframe\n",
    "for key, values in new_columns.items():\n",
    "    tile_pair_df_filtered[key] = values\n",
    "\n",
    "# Further filter tile pairs based on dice coefficient > 0.2 for better segmentation quality\n",
    "tile_pair_df_filtered_dicefilter = tile_pair_df_filtered[tile_pair_df_filtered[\"dice\"] > 0.2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067a0f6",
   "metadata": {},
   "source": [
    "Data loader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd929815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as geometric\n",
    "\n",
    "# Check if random cropping is enabled for data augmentation\n",
    "if config['crop']: ## we use this by default as our patches are 512 and we train with random 256*256 crops for better generalization\n",
    "    # Define training augmentation pipeline with cropping\n",
    "    train_transform = Compose([\n",
    "        geometric.RandomRotate90(),  # Randomly rotate images by 90 degrees\n",
    "        geometric.Flip(),  # Random horizontal/vertical flips\n",
    "        OneOf([  # Apply one of the following color augmentations\n",
    "            transforms.HueSaturationValue(),  # Adjust hue, saturation, and value\n",
    "            transforms.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2),  # Adjust contrast only\n",
    "            transforms.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0),  # Adjust brightness only\n",
    "        ], p=1),\n",
    "        geometric.RandomCrop(config['input_h'], config['input_w']),  # Random crop to target size\n",
    "        transforms.Normalize()  # Normalize pixel values\n",
    "    ],\n",
    "        is_check_shapes=False)\n",
    "\n",
    "    # Define validation transform with only cropping (no augmentation), also note that during validation we process \n",
    "    val_transform = Compose([\n",
    "        geometric.Resize(config['input_h'], config['input_w']),  # Resize to target dimensions\n",
    "        transforms.Normalize()  # Normalize pixel values\n",
    "    ],\n",
    "        is_check_shapes=False)\n",
    "\n",
    "else:\n",
    "    # Define training augmentation pipeline with resizing instead of cropping\n",
    "    train_transform = Compose([\n",
    "        geometric.RandomRotate90(),  # Randomly rotate images by 90 degrees\n",
    "        geometric.Flip(),  # Random horizontal/vertical flips\n",
    "        OneOf([  # Apply one of the following color augmentations\n",
    "            transforms.HueSaturationValue(),  # Adjust hue, saturation, and value\n",
    "            transforms.RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2),  # Adjust contrast only\n",
    "            transforms.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0),  # Adjust brightness only\n",
    "        ], p=1),\n",
    "        geometric.Resize(config['input_h'], config['input_w']),  # Resize to target dimensions\n",
    "        transforms.Normalize()  # Normalize pixel values\n",
    "    ],\n",
    "        is_check_shapes=False)\n",
    "\n",
    "    # Define validation transform with only resizing (no augmentation)\n",
    "    val_transform = Compose([\n",
    "        geometric.Resize(config['input_h'], config['input_w']),  # Resize to target dimensions\n",
    "        transforms.Normalize()  # Normalize pixel values\n",
    "    ],\n",
    "        is_check_shapes=False)\n",
    "\n",
    "# Create training dataset with augmentation and cell masking enabled\n",
    "train_dataset = HECOMETDataset_roi(\n",
    "        all_tile_pair=tile_pair_df,  # Complete tile pair dataframe\n",
    "        tile_pair_df=tile_pair_df_filtered_dicefilter,  # Filtered tile pairs based on quality metrics\n",
    "        transform=train_transform,  # Apply training augmentations\n",
    "        dir_path = config[\"tiling_dir\"],  # Path to tiled image directory\n",
    "        window_size = config[\"window_size\"],  # Size of image windows to extract\n",
    "        split=\"train\",  # Specify training split\n",
    "        mask_noncell=True,  # Mask non-cellular regions\n",
    "        cell_mask_label=True,  # Use cell mask labels\n",
    "    )    \n",
    "\n",
    "# Create validation dataset with minimal transforms\n",
    "val_dataset = HECOMETDataset_roi(\n",
    "    all_tile_pair=tile_pair_df,  # Complete tile pair dataframe\n",
    "    tile_pair_df=tile_pair_df_filtered_dicefilter,  # Filtered tile pairs based on quality metrics\n",
    "    transform=val_transform,  # Apply validation transforms (no augmentation)\n",
    "    dir_path = config[\"tiling_dir\"],  # Path to tiled image directory\n",
    "    window_size = config[\"window_size\"],  # Size of image windows to extract\n",
    "    split=\"valid\",  # Specify validation split\n",
    "    standard = \"silver\",  # this just sets the configration for validation tiles based on dice, \n",
    "    mask_noncell=True,  # Mask non-cellular regions\n",
    "    cell_mask_label=True,  # Use cell mask labels\n",
    ")    \n",
    "\n",
    "# Create training data loader with shuffling and parallel loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
    "                          num_workers=config['num_workers'], prefetch_factor=6, drop_last=True)\n",
    "\n",
    "# Create validation data loader without shuffling\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False,\n",
    "                        num_workers=config['num_workers'], prefetch_factor=6, drop_last=False)\n",
    "\n",
    "# Sample a subset of validation data for faster evaluation during training\n",
    "val_loader = sample_data_loader(val_loader, config, config['val_sampling_prob'], deterministic=True, what_split=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009f3ba",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6d9b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 879/879 [47:08<00:00,  3.22s/it, loss=0.788, pearson=0.287]\n",
      "100%|██████████| 4/4 [00:26<00:00,  6.60s/it, loss=0.863, pearson=0.239]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -> Loss: 0.7882, IoU: 0.2874\n",
      "Val   -> Loss: 0.8626, IoU: 0.2393\n",
      "End of Epoch 1\n",
      "Change settings (epochs) to train fully or use the train.py script to train the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"Epoch [{epoch+1}/{config['epochs']}]\")\n",
    "\n",
    "    # --- Train ---\n",
    "    train_log = train(config, train_loader, model, criterion, optimizer)\n",
    "\n",
    "    # --- Validate ---\n",
    "    val_log = validate(config, val_loader, model, criterion)\n",
    "\n",
    "    print(f\"Train -> Loss: {train_log['loss']:.4f}, IoU: {train_log['pearson']:.4f}\")\n",
    "    print(f\"Val   -> Loss: {val_log['loss']:.4f}, IoU: {val_log['pearson']:.4f}\")\n",
    "\n",
    "    print(\"End of Epoch 1\")\n",
    "    print(\"Change settings (epochs) to train fully or use the train.py script to train the model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
